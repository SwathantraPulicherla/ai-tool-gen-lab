#!/usr/bin/env python3
"""
CLI interface for AI C Test Generator
"""

import argparse
import os
import shutil
import sys
import time
from pathlib import Path

# Add compatibility for older Python versions
try:
    from importlib.metadata import packages_distributions
except ImportError:
    # Python < 3.10 compatibility
    try:
        from importlib_metadata import packages_distributions
    except ImportError:
        # Fallback implementation
        def packages_distributions():
            return {}

from .generator import SmartTestGenerator
from .validator import TestValidator


def _robust_cleanup_directory(dir_path, max_retries=3, delay=0.1):
    """
    Robustly clean up a directory on Windows, handling permission issues.
    
    Args:
        dir_path: Path to the directory to clean up
        max_retries: Maximum number of retries for each operation
        delay: Delay between retries in seconds
    """
    import time
    import stat
    
    if not os.path.exists(dir_path):
        return
    
    def _remove_readonly(func, path, _):
        """Clear readonly bit and reattempt removal"""
        try:
            os.chmod(path, stat.S_IWRITE)
            func(path)
        except OSError:
            pass
    
    # Try multiple strategies
    strategies = [
        # Strategy 1: Use shutil.rmtree with onerror handler
        lambda: shutil.rmtree(dir_path, onerror=_remove_readonly),
        
        # Strategy 2: Manual recursive removal with retries
        lambda: _manual_cleanup_with_retries(dir_path, max_retries, delay),
        
        # Strategy 3: Rename and recreate (last resort)
        lambda: _rename_and_recreate(dir_path)
    ]
    
    for strategy in strategies:
        try:
            strategy()
            return  # Success
        except Exception as e:
            print(f"[CLEAN] Strategy failed: {e}")
            continue
    
    # If all strategies fail, raise the last exception
    raise OSError(f"Failed to clean up directory {dir_path} after trying all strategies")


def _manual_cleanup_with_retries(dir_path, max_retries, delay):
    """Manually clean up directory with retries for each file/directory"""
    for root, dirs, files in os.walk(dir_path, topdown=False):
        # Remove files first
        for file in files:
            file_path = os.path.join(root, file)
            for attempt in range(max_retries):
                try:
                    if os.path.exists(file_path):  # Check if still exists
                        os.chmod(file_path, 0o666)  # Make writable
                        os.remove(file_path)
                    break
                except OSError:
                    if attempt < max_retries - 1:
                        time.sleep(delay)
                    else:
                        raise
        
        # Remove directories
        for dir_name in dirs:
            dir_full_path = os.path.join(root, dir_name)
            for attempt in range(max_retries):
                try:
                    if os.path.exists(dir_full_path):
                        os.rmdir(dir_full_path)
                    break
                except OSError:
                    if attempt < max_retries - 1:
                        time.sleep(delay)
                    else:
                        raise
    
    # Finally remove the root directory
    for attempt in range(max_retries):
        try:
            if os.path.exists(dir_path):
                os.rmdir(dir_path)
            break
        except OSError:
            if attempt < max_retries - 1:
                time.sleep(delay)
            else:
                raise


def _rename_and_recreate(dir_path):
    """Last resort: rename the directory and create a new one"""
    import tempfile
    import shutil
    
    # Generate a unique temporary name
    temp_name = None
    for i in range(100):  # Try up to 100 times
        try:
            temp_name = f"{dir_path}_old_{i}_{int(time.time())}"
            if not os.path.exists(temp_name):
                os.rename(dir_path, temp_name)
                break
        except OSError:
            continue
    else:
        raise OSError("Could not rename directory for cleanup")
    
    # Try to remove the renamed directory in background (don't wait)
    try:
        shutil.rmtree(temp_name)
    except OSError:
        # If we can't remove it, at least it's renamed out of the way
        pass


def create_parser():
    """Create argument parser for the CLI tool"""
    parser = argparse.ArgumentParser(
        description="AI-powered C unit test generator using Google Gemini",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generate tests for all C files in current directory
  ai-c-testgen --api-key YOUR_API_KEY

  # Generate tests for specific directory
  ai-c-testgen --repo-path /path/to/c/project --api-key YOUR_API_KEY

  # Use environment variable for API key
  export GEMINI_API_KEY=your_key_here
  ai-c-testgen --repo-path /path/to/c/project

  # Enable automatic regeneration for low-quality tests
  ai-c-testgen --repo-path /path/to/c/project --regenerate-on-low-quality --max-regeneration-attempts 3

  # Set quality threshold (only regenerate if below medium quality)
  ai-c-testgen --repo-path /path/to/c/project --regenerate-on-low-quality --quality-threshold medium
        """
    )

    parser.add_argument(
        '--repo-path',
        type=str,
        default='.',
        help='Path to the C repository (default: current directory)'
    )

    parser.add_argument(
        '--output',
        type=str,
        default='tests',
        help='Output directory for generated tests (default: tests)'
    )

    parser.add_argument(
        '--api-key',
        type=str,
        help='Google Gemini API key (can also use GEMINI_API_KEY env var)'
    )

    parser.add_argument(
        '--source-dir',
        type=str,
        default='src',
        help='Source directory containing C files (default: src)'
    )

    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Enable verbose output'
    )

    parser.add_argument(
        '--version',
        action='version',
        version='%(prog)s 1.0.0'
    )

    parser.add_argument(
        '--max-regeneration-attempts',
        type=int,
        default=2,
        help='Maximum number of regeneration attempts for low-quality tests (default: 2)'
    )

    parser.add_argument(
        '--regenerate-on-low-quality',
        action='store_true',
        help='Automatically regenerate tests that are validated as low quality'
    )

    parser.add_argument(
        '--redact-sensitive',
        action='store_true',
        help='Redact sensitive content (comments, strings, credentials) before sending to API'
    )

    parser.add_argument(
        '--quality-threshold',
        type=str,
        choices=['low', 'medium', 'high'],
        default='high',
        help='Quality threshold for regeneration (low, medium, high). Only regenerate tests below this threshold (default: high)'
    )

    return parser


def validate_environment(args):
    """Validate environment and arguments"""
    # Check repository path
    if not os.path.exists(args.repo_path):
        print(f"[ERROR] Repository path '{args.repo_path}' does not exist")
        return False

    # Check for C files in source directory
    source_path = os.path.join(args.repo_path, args.source_dir)
    if not os.path.exists(source_path):
        print(f"[ERROR] Source directory '{source_path}' does not exist")
        return False

    # Check for C files
    c_files = []
    for root, dirs, files in os.walk(source_path):
        for file in files:
            if file.endswith(('.c', '.h')):
                c_files.append(os.path.join(root, file))

    if not c_files:
        print(f"[ERROR] No C files found in '{source_path}'")
        return False

    # Check API key
    api_key = args.api_key or os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("[ERROR] Set GEMINI_API_KEY environment variable or use --api-key")
        print("   Get your API key from: https://makersuite.google.com/app/apikey")
        return False

    return True


def main():
    """Main CLI entry point"""
    parser = create_parser()
    args = parser.parse_args()

    if not validate_environment(args):
        sys.exit(1)

    api_key = args.api_key or os.getenv('GEMINI_API_KEY')

    print("[START] AI C Test Generator")
    print(f"   Repository: {args.repo_path}")
    print(f"   Source dir: {args.source_dir}")
    print(f"   Output dir: {args.output}")
    print()

    try:
        # Initialize components
        generator = SmartTestGenerator(api_key, args.repo_path, redact_sensitive=args.redact_sensitive)
        validator = TestValidator(args.repo_path)

        # Build dependency map
        if args.verbose:
            print("[BUILD] Building dependency map...")
        dependency_map = generator.build_dependency_map(args.repo_path)

        # Find C files in source directory (excluding main.c)
        source_path = os.path.join(args.repo_path, args.source_dir)
        c_files = []
        for root, dirs, files in os.walk(source_path):
            for file in files:
                if file.endswith('.c'):  # Only process .c files, not headers
                    # Skip main.c as it's not suitable for unit testing
                    if file == 'main.c':
                        if args.verbose:
                            print(f"[SKIP] Skipping main.c (application entry point)")
                        continue
                    c_files.append(os.path.join(root, file))

        if args.verbose:
            print(f"[FOUND] Found {len(c_files)} C files to process")

        # Create output directory
        output_dir = os.path.join(args.repo_path, args.output)
        os.makedirs(output_dir, exist_ok=True)

        # Clean up old compilation reports
        compilation_report_dir = os.path.join(output_dir, "compilation_report")
        if os.path.exists(compilation_report_dir):
            print("[CLEAN] Cleaning up old compilation reports...")
            try:
                import shutil
                shutil.rmtree(compilation_report_dir)
                print("[CLEAN] Cleanup completed successfully")
            except (OSError, PermissionError) as e:
                print(f"[WARN] Standard cleanup failed: {e}")
                # Try robust Windows-compatible cleanup
                try:
                    _robust_cleanup_directory(compilation_report_dir)
                    print("[CLEAN] Robust cleanup completed")
                except Exception as cleanup_error:
                    print(f"[WARN] Robust cleanup also failed: {cleanup_error}")
                    print("[WARN] Skipping cleanup - old reports may remain")
        os.makedirs(compilation_report_dir, exist_ok=True)

        # Process each file
        successful_generations = 0
        validation_reports = []
        regeneration_stats = {'total_regenerations': 0, 'successful_regenerations': 0}

        for file_path in c_files:
            rel_path = os.path.relpath(file_path, args.repo_path)
            file_start_time = time.time()
            print(f"[PROC] Processing: {rel_path}")

            max_attempts = args.max_regeneration_attempts + 1  # +1 for initial generation
            attempt = 0
            final_result = None
            final_validation = None

            while attempt < max_attempts:
                attempt += 1
                try:
                    # Generate tests for this file
                    if args.verbose:
                        print(f"   [GEN] Starting AI test generation for {os.path.basename(file_path)}...")
                    result = generator.generate_tests_for_file(
                        file_path, args.repo_path, output_dir, dependency_map, final_validation if attempt > 1 else None
                    )
                    if args.verbose and result['success']:
                        print(f"   [GEN] AI generation completed, post-processing...")

                    if not result['success']:
                        print(f"   [ERROR] Generation failed: {result['error']}")
                        break

                    # Validate the generated test
                    if args.verbose:
                        print(f"   [VALIDATE] Validating (attempt {attempt})...")
                    validation_result = validator.validate_test_file(result['test_file'], file_path)
                    if args.verbose:
                        print(f"   [VALIDATE] Validation completed: {validation_result['quality']} quality")

                    # Check if regeneration is needed based on quality threshold
                    quality_levels = {'low': 0, 'medium': 1, 'high': 2}
                    current_quality_level = quality_levels.get(validation_result['quality'].lower(), 0)
                    threshold_quality_level = quality_levels.get(args.quality_threshold.lower(), 0)

                    needs_regeneration = (
                        args.regenerate_on_low_quality and
                        current_quality_level < threshold_quality_level and
                        attempt < max_attempts
                    )

                    # Print validation summary
                    status = "[OK]" if validation_result['compiles'] and validation_result['realistic'] else "[WARN]"
                    quality = validation_result['quality']
                    compiles = 'Compiles' if validation_result['compiles'] else 'Broken'
                    realistic = 'Realistic' if validation_result['realistic'] else 'Unrealistic'

                    if attempt == 1:
                        print(f"   {status} {quality} quality ({compiles}, {realistic})")
                    else:
                        print(f"   {status} {quality} quality ({compiles}, {realistic}) - regenerated")

                    if not validation_result['compiles'] and validation_result['issues']:
                        print(f"   Issues: {len(validation_result['issues'])}")
                        if args.verbose:
                            for issue in validation_result['issues'][:3]:  # Show first 3 issues
                                print(f"     - {issue}")

                    # Store final results
                    final_result = result
                    final_validation = validation_result

                    # Check if we should regenerate
                    if needs_regeneration:
                        print(f"   [REGEN] Low quality detected, regenerating (attempt {attempt + 1}/{max_attempts})...")
                        regeneration_stats['total_regenerations'] += 1
                        # Remove the low-quality test file so it can be regenerated
                        if os.path.exists(result['test_file']):
                            os.remove(result['test_file'])
                        continue
                    else:
                        # Quality is acceptable or we've reached max attempts
                        break

                except Exception as e:
                    print(f"   [ERROR] Error processing {rel_path}: {str(e)}")
                    break

            # Process final result
            if final_result and final_result['success']:
                successful_generations += 1
                validation_reports.append(final_validation)

                # Track successful regenerations
                if attempt > 1:
                    regeneration_stats['successful_regenerations'] += 1

                print(f"   [FINAL] Final: {os.path.basename(final_result['test_file'])} ({final_validation['quality']} quality)")
            else:
                print(f"   [ERROR] Failed to generate acceptable test for {rel_path}")

            # Print timing for this file
            file_duration = time.time() - file_start_time
            print(f"   [TIME] Completed in {file_duration:.1f}s")

        # Save validation reports
        if validation_reports:
            print(f"\n[SAVE] Saving validation reports...")
            report_dir = os.path.join(args.repo_path, args.output, "compilation_report")

            for report in validation_reports:
                validator.save_validation_report(report, report_dir)

        # Print summary
        print(f"\n[DONE] COMPLETED!")
        print(f"   Generated: {successful_generations}/{len(c_files)} files")
        print(f"   Tests saved to: {output_dir}")
        if validation_reports:
            print(f"   Reports saved to: {os.path.join(args.output, 'compilation_report')}")

        # Print regeneration statistics
        if args.regenerate_on_low_quality:
            print(f"   Regenerations: {regeneration_stats['successful_regenerations']}/{regeneration_stats['total_regenerations']} successful")
            if regeneration_stats['total_regenerations'] > 0:
                success_rate = (regeneration_stats['successful_regenerations'] / regeneration_stats['total_regenerations']) * 100
                print(f"   Regeneration success rate: {success_rate:.1f}%")

        # Check quality of all generated tests
        quality_levels = {'low': 0, 'medium': 1, 'high': 2}
        threshold_quality_level = quality_levels.get(args.quality_threshold.lower(), 2)

        low_quality_tests = []
        for report in validation_reports:
            current_quality_level = quality_levels.get(report['quality'].lower(), 0)
            if current_quality_level < threshold_quality_level:
                low_quality_tests.append(report['file'])

        # Check quality of all generated tests
        quality_levels = {'low': 0, 'medium': 1, 'high': 2}
        threshold_quality_level = quality_levels.get(args.quality_threshold.lower(), 2)

        low_quality_tests = []
        for report in validation_reports:
            current_quality_level = quality_levels.get(report['quality'].lower(), 0)
            if current_quality_level < threshold_quality_level:
                low_quality_tests.append(report['file'])

        if low_quality_tests:
            if args.regenerate_on_low_quality:
                # When regeneration is enabled, warn but don't fail
                print(f"[WARN] {len(low_quality_tests)} test(s) still below {args.quality_threshold} quality threshold after regeneration:")
                for test_file in low_quality_tests:
                    print(f"   - {test_file}")
                print("[TIP] Consider increasing --max-regeneration-attempts or relaxing --quality-threshold")
            else:
                # When regeneration is disabled, strict enforcement
                print(f"[ERROR] {len(low_quality_tests)} test(s) failed to meet {args.quality_threshold} quality threshold:")
                for test_file in low_quality_tests:
                    print(f"   - {test_file}")
                print("[TIP] Use --regenerate-on-low-quality to automatically improve test quality")
                sys.exit(1)

        # Overall success check
        if successful_generations == 0:
            print("[ERROR] No tests were successfully generated")
            sys.exit(1)
        elif successful_generations < len(c_files):
            print("[WARN] Some files failed to generate tests - check validation reports")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n[STOP] Interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"[ERROR] Fatal error: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
